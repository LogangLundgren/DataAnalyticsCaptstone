# -*- coding: utf-8 -*-
"""Exercise Detection Model Random Forest.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hq-1VE7XUh-po8Ha4q0dzazgT12H6xD5
"""
"""
LIBRARIES EXPLANATION:
- pandas: handles our data in table format
- numpy: performs mathematical calculations
- scikit-learn: contains our machine learning tools
- seaborn/matplotlib: creates visualizations
- supabase: connects to our database
"""

!pip install pandas numpy scikit-learn seaborn matplotlib

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder
import seaborn as sns
import matplotlib.pyplot as plt
from supabase import create_client, Client

"""
1. load_data_from_supabase():
   - Purpose: Retrieves our exercise data from the Supabase database
   - Process:
     * Connects to the "video_frames" table
     * Selects all columns (*)
     * Converts the response into a pandas DataFrame 
"""

def load_data_from_supabase():
    response = supabase.table("video_frames").select("*").execute()
    df = pd.DataFrame(response.data)
    return df

def extract_exercise_type(filename):

    return filename.split('_')[0].lower()


"""
Connection Components:
1. SUPABASE_URL:
   - The address of our database

2. SUPABASE_KEY:
   - Our secure access token
   - Important: Never share this key publicly!

3. create_client():
   - Creates our connection to the database
   - Handles all communication between our code and the database
"""


# Supabase connection
SUPABASE_URL = "https://hrtjlralgdecioejfqct.supabase.co"
SUPABASE_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImhydGpscmFsZ2RlY2lvZWpmcWN0Iiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDIzMjg0NDEsImV4cCI6MjA1NzkwNDQ0MX0.Psth4TM1dsMsb4Yxrpfku4LK938H4HL2ORGAmhOVMSU"
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)


    """
    Extract exercise type from source_video string
    """
def extract_exercise_type(source_video):

    # Handle special cases
    if 'Tutorial' in source_video:
        return 'barbell hip thrust'

"""
What It Does:
1. Takes a video filename (source_video)
2. Extracts the exercise name while handling different formats

How It Works:
- splits the filename at underscores ('_')
- Example 1: "bench_press_1" → ["bench", "press", "1"]
- Example 2: "bicep_curl_set_1" → ["bicep", "curl", "set", "1"]
"""

    # Normal cases: split by underscore or last occurrence of space+number
    parts = source_video.split('_')
    if len(parts) > 1:
        # Join all parts except the last one (which is the number)
        return ' '.join(parts[:-1]).lower()
    return source_video.lower()


Python

Collapse
"""
Why Batch Loading?
- We have over 112,000 frames of exercise data
- Loading all at once could crash our program

Key Components:
1. Batch Configuration:
   - batch_size = 1000: Process 1000 rows at a time
   - offset: Keeps track of where we are in the data
   - all_data: Collects all our batches

2. Loading Loop:
   - Starts at offset = 0
   - Loads next 1000 rows
   - Updates offset
   - Continues until no more data
"""

def load_data_from_supabase():
    print("Loading data from Supabase...")
    # Load data in batches to handle large dataset
    offset = 0
    batch_size = 1000
    all_data = []

    while True:
        response = supabase.table("video_frames")\
            .select("*")\
            .range(offset, offset + batch_size - 1)\
            .execute()

        if not response.data:
            break

        all_data.extend(response.data)
        offset += batch_size
        print(f"Loaded {len(all_data)} rows...")

    return pd.DataFrame(all_data)

"""
This function prepares our raw data for machine learning:

Key Steps:
1. Feature Selection:
   - Identifies columns containing body position data
   - Each landmark has 4 values: x, y, z coordinates and visibility
   - Example: 33 landmarks × 4 values = 132 features
   - Code: [col for col in df.columns if any(x in col for x in ['x', 'y', 'z', 'visibility'])]

2. Exercise Type Extraction:
   - Creates new column 'exercise_type'
   - Uses our earlier extract_exercise_type function
   - Standardizes all exercise names

3. Data Cleaning:
   - Removes rows with missing values
   - Ensures quality data for training
   - Prevents errors in model training
   - Code: df.dropna(subset=feature_cols)

4. Statistics Reporting:
   - Shows distribution of exercise types
   - Reports total number of frames
   - Counts unique exercises
   - Helps identify data imbalances

Output:
- Clean DataFrame ready for model training
- List of feature columns for training
- Summary statistics of our dataset

"""

def preprocess_data(df):
    # Get feature columns (x, y, z, visibility for each landmark)
    feature_cols = [col for col in df.columns if any(x in col for x in ['x', 'y', 'z', 'visibility'])]

    # Extract exercise types
    df['exercise_type'] = df['source_video'].apply(extract_exercise_type)

    # Drop rows with missing values
    df = df.dropna(subset=feature_cols)

    # Print data statistics
    print("\nExercise types found:")
    exercise_counts = df['exercise_type'].value_counts()
    print(exercise_counts)
    print(f"\nTotal frames: {len(df)}")
    print(f"Number of exercises: {len(exercise_counts)}")

    return df, feature_cols

def calculate_joint_angles(df):
    """Calculate angles between joints"""
    def calculate_angle(p1, p2, p3):
        try:
            v1 = np.array([p1[0]-p2[0], p1[1]-p2[1], p1[2]-p2[2]])
            v2 = np.array([p3[0]-p2[0], p3[1]-p2[1], p3[2]-p2[2]])

            cosine = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
            angle = np.arccos(np.clip(cosine, -1.0, 1.0))
            return np.degrees(angle)
        except:
            return np.nan

    print("Calculating joint angles...")

    # Calculate right elbow angle
    df['right_elbow_angle'] = df.apply(lambda row: calculate_angle(
        (row['x11'], row['y11'], row['z11']),  # shoulder
        (row['x13'], row['y13'], row['z13']),  # elbow
        (row['x15'], row['y15'], row['z15'])   # wrist
    ), axis=1)

    # Calculate left elbow angle
    df['left_elbow_angle'] = df.apply(lambda row: calculate_angle(
        (row['x12'], row['y12'], row['z12']),  # shoulder
        (row['x14'], row['y14'], row['z14']),  # elbow
        (row['x16'], row['y16'], row['z16'])   # wrist
    ), axis=1)

    # Calculate right knee angle
    df['right_knee_angle'] = df.apply(lambda row: calculate_angle(
        (row['x23'], row['y23'], row['z23']),  # hip
        (row['x25'], row['y25'], row['z25']),  # knee
        (row['x27'], row['y27'], row['z27'])   # ankle
    ), axis=1)

    # Calculate left knee angle
    df['left_knee_angle'] = df.apply(lambda row: calculate_angle(
        (row['x24'], row['y24'], row['z24']),  # hip
        (row['x26'], row['y26'], row['z26']),  # knee
        (row['x28'], row['y28'], row['z28'])   # ankle
    ), axis=1)

    return df

def train_and_evaluate_model(X, y):
    # Split the data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # Train Random Forest
    print("Training Random Forest model...")
    rf_model = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        random_state=42,
        n_jobs=-1
    )

    rf_model.fit(X_train, y_train)

    # Evaluate
    y_pred = rf_model.predict(X_test)

    return rf_model, X_test, y_test, y_pred

def plot_results(model, X_test, y_test, y_pred, feature_cols, le):
    # Plot confusion matrix
    plt.figure(figsize=(15, 10))
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(
        cm,
        annot=True,
        fmt='d',
        xticklabels=le.classes_,
        yticklabels=le.classes_
    )
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()

    # Plot feature importance
    feature_importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)

    plt.figure(figsize=(15, 8))
    sns.barplot(data=feature_importance.head(20), x='importance', y='feature')
    plt.title('Top 20 Most Important Features')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

def main():
    # Load data
    df = load_data_from_supabase()

    # Preprocess
    print("Preprocessing data...")
    df, feature_cols = preprocess_data(df)

    # Feature engineering
    df = calculate_joint_angles(df)
    angle_cols = ['right_elbow_angle', 'left_elbow_angle', 'right_knee_angle', 'left_knee_angle']
    feature_cols.extend(angle_cols)

    # Prepare features and labels
    X = df[feature_cols]
    le = LabelEncoder()
    y = le.fit_transform(df['exercise_type'])

    # Train and evaluate
    model, X_test, y_test, y_pred = train_and_evaluate_model(X, y)

    # Print classification report
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=le.classes_))

    # Plot results
    plot_results(model, X_test, y_test, y_pred, feature_cols, le)

    return model, le, df

if __name__ == "__main__":
    model, label_encoder, final_df = main()




"""
DATA COLLECTION:
- We have 112,427 frames from exercise videos
- Each frame has 33 body landmarks (x, y, z coordinates + visibility)
- Data comes from MediaPipe pose detection
- Stored in Supabase database for easy access
"""

    """
    def extract_exercise_type(source_video):

    PREPROCESSING STEP 1: LABELING
    - Extracts exercise type from video names
    - Handles special cases and formatting
    - Important because good labels = good training

    LIMITATION:
    - Relies on consistent video naming
    - Could be improved with manual verification
    """

    """
    def load_data_from_supabase():

    DATA LOADING:
    - Loads data in batches to handle large dataset
    - 1000 rows at a time to manage memory
    - Shows progress during loading

    LIMITATION:
    - Could be slow with very large datasets
    - Network dependency
    """

    """
    def preprocess_data(df):

    PREPROCESSING STEP 2: DATA CLEANING
    - Identifies relevant features (body positions)
    - Removes incomplete or corrupted data
    - Creates exercise type labels

    WHY THIS MATTERS:
    - Clean data is crucial for accurate predictions
    - Helps us understand our dataset composition
    """

    """
    def calculate_joint_angles(df):

    FEATURE ENGINEERING:
    - Calculates angles between joints
    - Examples: elbow angle, knee angle
    - Uses vector mathematics for accuracy

    WHY THIS HELPS:
    - Adds meaningful features that describe movement
    - Makes exercise recognition more accurate
    - Less dependent on camera position

    LIMITATION:
    - Assumes joints are correctly detected
    - Could add more complex biomechanical features
    """

    """
    def train_and_evaluate_model(X, y):

    MODEL TRAINING:
    - Splits data into training (80%) and testing (20%)
    - Creates 100 decision trees
    - Each tree learns slightly different patterns

    RANDOM FOREST PARAMETERS:
    - n_estimators=100: number of trees
    - max_depth=10: prevents overly complex trees
    - n_jobs=-1: uses all CPU cores

    LIMITATIONS:
    - Treats each frame independently
    - Doesn't consider exercise sequence
    - Could be improved with temporal features
    """

    """
    def plot_results(model, X_test, y_test, y_pred, feature_cols, le):

    VISUALIZATION AND ANALYSIS:
    - Confusion matrix shows where model makes mistakes
    - Feature importance shows which body positions matter most

    INTERPRETING RESULTS:
    - 97% overall accuracy
    - Some exercises easier to recognize than others
    - Helps identify where model needs improvement
    """


    """
    def main():
    COMPLETE PIPELINE:
    1. Load data from database
    2. Clean and preprocess
    3. Engineer features
    4. Train model
    5. Evaluate results

    FUTURE IMPROVEMENTS:
    1. Add temporal features (movement over time)
    2. Include more joint angles and relationships
    3. Use deep learning for sequence analysis
    4. Add real-time prediction capabilities
    5. Handle multiple people in frame
    """

"""
CONCLUSION:
- Model successfully classifies 22 different exercises
- 97% accuracy on test data
- Best at: Plank, Push-up (100% accuracy)
- Most challenging: Romanian Deadlift (88% accuracy)

KEY STRENGTHS:
1. Handles variety of exercises
2. Provides interpretable results
3. Fast prediction time
4. Robust to different body types

LIMITATIONS:
1. Frame-by-frame analysis
2. Requires good pose detection
3. Doesn't consider exercise form quality
4. May struggle with similar exercises

FUTURE APPLICATIONS:
1. Real-time exercise feedback
2. Automated workout logging
3. Form correction
4. Exercise recommendation
"""
